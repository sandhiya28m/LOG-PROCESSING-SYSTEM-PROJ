# Distributed Log Processing System
A Python-based distributed log processing system built using PySpark and Streamlit.
This system processes large-scale log files, performs advanced analytics, generates automated reports, and provides an interactive dashboard for real-time visualization.

ğŸ¯ Features

Distributed Processing â€“ Scalable log processing using PySpark

Comprehensive Analytics â€“ Error trends, severity analysis, IP & service breakdown

Alert System â€“ Configurable threshold-based alerts

Interactive Dashboard â€“ Streamlit-based real-time visualization

Automated Reporting â€“ CSV & JSON report generation

Production Ready â€“ Modular, clean, and scalable architecture

ğŸ“‚ Project Structure
distributed-log-system/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw_logs/                 # Input CSV log files
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ csv/                      # CSV reports
â”‚   â”œâ”€â”€ json/                     # JSON reports
â”‚   â””â”€â”€ alerts.log                # Alert history
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ spark/
â”‚   â”‚   â”œâ”€â”€ spark_session.py      # Spark session management
â”‚   â”‚   â”œâ”€â”€ ingest_logs.py        # Log ingestion
â”‚   â”‚   â”œâ”€â”€ parse_logs.py         # Log parsing & normalization
â”‚   â”‚   â”œâ”€â”€ analytics.py          # Core analytics
â”‚   â”‚   â”œâ”€â”€ alerts.py             # Alert system
â”‚   â”‚   â””â”€â”€ export_reports.py     # Report generation
â”‚   â”‚
â”‚   â”œâ”€â”€ dashboard/
â”‚   â”‚   â””â”€â”€ app.py                # Streamlit dashboard
â”‚   â”‚
â”‚   â””â”€â”€ main.py                   # Main processing pipeline
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml               # Configuration file
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸš€ Setup
Prerequisites

Python 3.8+

Java 8+ (Required for PySpark)

Installation

Clone the repository:

git clone <repository-url>
cd distributed-log-system


Create and activate a virtual environment:

python -m venv venv
source venv/bin/activate     # Windows: venv\Scripts\activate


Install dependencies:

pip install -r requirements.txt


Verify Java installation:

java -version

ğŸ“Š Input Data Format

Place CSV log files inside:

data/raw_logs/

Required Columns

timestamp â€“ Log timestamp (multiple formats supported)

log_level â€“ INFO, WARN, ERROR, DEBUG

message â€“ Log message

Optional Columns

ip or IP â€“ IP address

service â€“ Service name

endpoint â€“ API endpoint

Example CSV
timestamp,log_level,message,ip,service
2024-01-15 10:30:00,INFO,Request processed successfully,192.168.1.100,api-service
2024-01-15 10:31:00,ERROR,Connection timeout exception,192.168.1.101,api-service
2024-01-15 10:32:00,WARN,High response time detected,192.168.1.102,web-service

ğŸ”§ Configuration

Edit config/config.yaml to customize:

Spark Settings â€“ Memory, executors, master

Input/Output Paths

Alert Thresholds

Analytics Parameters

Dashboard Auto-refresh Settings

ğŸƒ Running the System
1ï¸âƒ£ Run Full Pipeline
python src/main.py

2ï¸âƒ£ Run Individual Modules (Optional)
python src/spark/ingest_logs.py
python src/spark/parse_logs.py
python src/spark/analytics.py
python src/spark/alerts.py
python src/spark/export_reports.py

ğŸ“Š Launch Dashboard

Start the Streamlit dashboard:

streamlit run src/dashboard/app.py


Access at:
ğŸ‘‰ http://localhost:8501

ğŸ“ˆ Dashboard Features

Key Metrics

Total logs

Total errors

Error rate

Active alerts

Visualizations

Errors over time (line chart)

Top error types (bar chart)

Errors by IP

Errors by service

Filters

Date range

Log level selection

Alert Panel

Recent alerts with timestamps

Auto Refresh

Live dashboard updates

ğŸ“‹ Generated Reports
CSV Reports (reports/csv/)

errors_by_type.csv

errors_by_hour.csv

top_errors.csv

errors_per_ip.csv

errors_per_service.csv

JSON Reports (reports/json/)

error_trends.json

errors_by_day.json

errors_by_severity.json

Alert Log

reports/alerts.log

ğŸ”” Alert System

Alerts are triggered for:

High Error Rate (default: >10%)

High Error Count (default: >100)

Critical Errors (default: >5)

Alerts are:

Logged to file

Printed to console

Displayed in dashboard

ğŸ› ï¸ Development Guidelines

No Pandas for Processing â€“ All transformations use PySpark

Modular Codebase â€“ Easy to extend analytics & alerts

Config-Driven â€“ YAML-based configuration

Caching & Optimization â€“ Spark caching enabled

Adding New Analytics

Add logic in analytics.py

Call function in run_all_analytics()

Export results via export_reports.py

Visualize in dashboard/app.py

ğŸš€ Deployment
Local

Uses local[*] Spark master by default

Cluster Deployment

Update config.yaml:

spark:
  master: spark://your-cluster:7077


Supports:

AWS EMR

Databricks

On-prem Spark clusters

ğŸ› Troubleshooting
Java Not Found

Install Java 8+

Set JAVA_HOME

Spark Errors

Verify Java installation

Check Spark configs

Dashboard Not Updating

Ensure pipeline ran successfully

Verify report paths

ğŸ“„ License

This project is provided as-is for educational and development purposes.

ğŸ‘¥ Contributing

Contributions are welcome!

You can enhance this project by adding:

Advanced analytics

New visualizations

Enhanced alerting logic

Performance optimizations

ğŸš€ Built with PySpark & Streamlit
